import torchimport torch.nn as nnimport torch.optim as optimfrom sklearn.cluster import KMeansimport numpy as npimport timeclass Generator(nn.Module):    def __init__(self, input_dim, output_dim):        super(Generator, self).__init__()        self.net = nn.Sequential(            nn.Linear(input_dim, 64),            nn.ReLU(),            nn.Linear(64, output_dim),            nn.Tanh()        )    def forward(self, z):        return self.net(z)class Encoder(nn.Module):    def __init__(self, input_dim, latent_dim):        super(Encoder, self).__init__()        self.net = nn.Sequential(            nn.Linear(input_dim, 64),            nn.ReLU(),            nn.Linear(64, latent_dim)        )    def forward(self, x):        return self.net(x)def clustergan_cluster(X, latent_dim=10, epochs=100, n_clusters=10):    start = time.time()    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)    dataset = torch.utils.data.TensorDataset(X_tensor)    loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True)    G = Generator(latent_dim, X.shape[1]).to(device)    E = Encoder(X.shape[1], latent_dim).to(device)    opt_G = optim.Adam(G.parameters(), lr=1e-3)    opt_E = optim.Adam(E.parameters(), lr=1e-3)    mse = nn.MSELoss()    for epoch in range(epochs):        for batch in loader:            x_batch = batch[0]            z = E(x_batch)            x_recon = G(z)            loss = mse(x_recon, x_batch)            opt_G.zero_grad()            opt_E.zero_grad()            loss.backward()            opt_G.step()            opt_E.step()    E.eval()    with torch.no_grad():        Z = E(X_tensor).cpu().numpy()    y_pred = KMeans(n_clusters=n_clusters, n_init=10).fit_predict(Z)    end = time.time()    return y_pred, start, end