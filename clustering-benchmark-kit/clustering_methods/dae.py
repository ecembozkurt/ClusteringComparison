import torchimport torch.nn as nnimport torch.optim as optimfrom sklearn.cluster import KMeansimport numpy as npimport timeclass DAE(nn.Module):    def __init__(self, input_dim, hidden_dim=64):        super(DAE, self).__init__()        self.encoder = nn.Sequential(            nn.Linear(input_dim, hidden_dim),            nn.ReLU()        )        self.decoder = nn.Sequential(            nn.Linear(hidden_dim, input_dim),            nn.Sigmoid()        )    def forward(self, x):        encoded = self.encoder(x)        decoded = self.decoder(encoded)        return decoded, encodeddef dae_cluster(X, n_clusters=10, epochs=50, batch_size=128, hidden_dim=64):    start = time.time()    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)    dataset = torch.utils.data.TensorDataset(X_tensor)    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)    model = DAE(input_dim=X.shape[1], hidden_dim=hidden_dim).to(device)    criterion = nn.MSELoss()    optimizer = optim.Adam(model.parameters(), lr=1e-3)    model.train()    for epoch in range(epochs):        for batch in loader:            x_batch = batch[0]            optimizer.zero_grad()            x_recon, _ = model(x_batch)            loss = criterion(x_recon, x_batch)            loss.backward()            optimizer.step()    model.eval()    with torch.no_grad():        _, embeddings = model(X_tensor)        embeddings_np = embeddings.cpu().numpy()    y_pred = KMeans(n_clusters=n_clusters, n_init=10).fit_predict(embeddings_np)    end = time.time()    return y_pred, start, end